# -*- coding: utf-8 -*-
"""Final_IDS_575_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RcLO5DafBRtiPXT4oJyViPL-Ch1wM39r
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import matplotlib
import sklearn
from sklearn import svm
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,classification_report,confusion_matrix
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.preprocessing import normalize
import seaborn as sns
from sklearn import linear_model
from sklearn.metrics import auc, plot_precision_recall_curve


df=pd.read_csv("/content/drive/MyDrive/Epileptic Seizure Recognition.csv")
df

!pip install sklearn

print(f"total instances and attributes: {df.shape}")

df['y'].value_counts()

df.isna().sum()

del df['Unnamed']

co_matrix = df.corr()
co_matrix.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)

co_matrix = df.corr()
import seaborn as sns
sns.heatmap(co_matrix, cmap=sns.diverging_palette(220, 10, as_cmap=True))

df_new=df.copy()

plt.bar(df_new.X1,df_new.y)
plt.show()

plt.bar(df_new.X100,df_new.y)
plt.show()

plt.bar(df_new.y,df_new.X99)
plt.show()

plt.barh(df_new.X1,df_new.y)
plt.show()

df['y'] = df['y'].apply(lambda x: 1 if x == 1 else 0)
df

plt.bar(df_new.X26,df_new.y==1)
plt.show()

plt.bar(df_new.X160,df_new.y==1)
plt.show()

plt.scatter(df.X1,df.y)
plt.show()

plt.scatter(df.X1,df.X2,df.X3,df.y)
#df.X1,df.X2,df.X3,df.X4,df.X5,df.X6,df.X7,df.X8,df.X9,df.X10,df.X11,df.X12,df.X13,df.X14,df.X15,df.X16,df.X17,df.X18,df.X19,df.X20,df.X21,df.X22,df.X23,df.X24,df.X25,df.X26,df.X27,df.X28,df.X29,df.X30,df.X31,df.X32,df.X33,df.X34,df.X35,df.X36,df.X37,df.X38,df.X39,df.X40,df.X41,df.X42,df.X43,df.X44,df.X45,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X2,df.X80
plt.show()

plt.scatter(df.y,df.X99)
plt.show()

plt.bar(df.y,df.X1)
plt.show()

"""### Baseline model"""

# Show counterplot of 'y' feature
import seaborn as sns
data=df.y
sns.countplot(df.y)
df['y'].value_counts()

# # copy the data
# df_min_max_scaled = df.copy()

# # apply normalization techniques
# for column in df_min_max_scaled.columns:
#     df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())

# # view normalized data
# df_min_max_scaled

# df_min_max_scaled.describe()

co_matrix = df.corr()
import seaborn as sns
sns.heatmap(co_matrix, cmap=sns.diverging_palette(220, 10, as_cmap=True))

import sys
df_min_max_scaled = df.copy()
df_min_max_scaled[df_min_max_scaled>1]=0
X = df_min_max_scaled.loc[:, df_min_max_scaled.columns != 'y']
y = df_min_max_scaled.loc[:, df_min_max_scaled.columns == 'y']

for column in X.columns:
  X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min())

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)

df.y.value_counts(normalize=True)

# Select Baseline Model as Simple Logistic Regression

logr = linear_model.LogisticRegression()
logr.fit(X_train,y_train.values.ravel())

predicted = logr.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)
Base=recall_score(y_test, predicted)
print('Accuracy: %.3f' % accuracy_score(y_test, predicted))
print('Recall: %.3f' % recall_score(y_test, predicted))
print('Precision: %.3f' % precision_score(y_test, predicted))
print(classification_report(y_test,predicted))

from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import numpy as np

x = np.linspace(0,1,100)
# train, test, train_t, test_t = train_test_split(X, y, train_size=0.8)
# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)
plt.plot(*roc_curve(y_test, logr.predict_proba(X_test)[:,1])[:2])
plt.plot(*roc_curve(y_test, logr.predict(X_test))[:2])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.plot(x, x, '-r')
plt.show()

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, logr.predict_proba(X_test)[:,1])
print(f"AUC:{auc}")

"""### **PR-Curve**"""

from sklearn.metrics import precision_recall_curve,auc, plot_precision_recall_curve
lr_precision, lr_recall,threshold = precision_recall_curve(y_test, logr.predict_proba(X_test)[:,1])
auc_precision_recall = auc(lr_recall, lr_precision)
print(auc_precision_recall)
plt.plot(lr_recall, lr_precision, marker='.',)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show()

"""### We used SMOTE to create a balanced dataset because our dataset was imbalanced."""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

os = SMOTE(random_state=0)
columns = X_train.columns

os_data_X,os_data_y=os.fit_resample(X_train, y_train)
os_data_X = pd.DataFrame(data=os_data_X,columns=columns )
os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])

print("length of oversampled data is ",len(os_data_X))
print("Number of no Epileptic Seizure in oversampled data",len(os_data_y[os_data_y['y']==0]))
print("Number of Epileptic Seizure",len(os_data_y[os_data_y['y']==1]))
print("Proportion of no Epileptic Seizure data in oversampled data is ",len(os_data_y[os_data_y['y']==0])/len(os_data_X))
print("Proportion of Epileptic Seizure data in oversampled data is ",len(os_data_y[os_data_y['y']==1])/len(os_data_X))

os_data_y.value_counts()

"""### **1. Logistic Regression**"""

from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
import warnings

warnings.filterwarnings('ignore')

def k(k):
    kf = model_selection.KFold(n_splits=k, shuffle=True)
    model = LogisticRegression(n_jobs=-1)
    #result = cross_val_score(model, os_data_X, os_data_y, cv = kf, n_jobs = -1)
    mean_score = cross_val_score(model, os_data_X, os_data_y, scoring="recall", cv = kf,n_jobs = -1)
    print("Avg Recall: {:.4f}".format(mean_score.mean()))
    #print("Avg accuracy: {:.4f}".format(result.mean()))

lis = [5,7,9,13]
for x in range(len(lis)):
    k(lis[x])

from sklearn.linear_model import LogisticRegression
model=LogisticRegression(n_jobs=-1)
model.fit(os_data_X, os_data_y)
threshold = 0.5
y_pred=model.predict(X_test)

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)
logistic=recall_score(y_test, y_pred)
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))
print(classification_report(y_test,y_pred))

"""### **ROC**"""

x = np.linspace(0,1,100)
plt.plot(*roc_curve(y_test, model.predict_proba(X_test)[:,1])[:2])
plt.plot(*roc_curve(y_test, model.predict(X_test))[:2])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.plot(x, x, '-r')
plt.show()

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])
print(f"AUC:{auc}")

"""### **PR CURVE**"""

from sklearn.metrics import precision_recall_curve,auc
lr_precision, lr_recall,threshold = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])
auc_precision_recall = auc(lr_recall, lr_precision)
print(auc_precision_recall)
plt.plot(lr_recall, lr_precision, marker='.',)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show()

"""## SVM

### *Soft Margin SVM*

### **RBF Kernel**
"""

# import SVC classifier
from sklearn.svm import SVC

svc=SVC(probability=True)

# fit classifier to training set
svc.fit(os_data_X,os_data_y.values.ravel())

# make predictions on test set
y_pred=svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)

# compute and print accuracy score
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
print('Model recall score with default hyperparameters: {0:0.4f}'. format(recall_score(y_test, y_pred)))
print(conf_matrix)

conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()
SVM_RBF= recall_score(y_test, y_pred)
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

from sklearn.metrics import precision_recall_curve,auc
svm_precision, svm_recall,threshold = precision_recall_curve(y_test, svc.predict_proba(X_test)[:,1])
auc_precision_recall = auc(svm_recall, svm_precision)
print(f"AUC-PR: {auc_precision_recall}")
plt.plot(svm_recall, svm_precision, marker='.',)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show()

lr_precision, lr_recall,threshold = precision_recall_curve(y_test, svc.predict_proba(X_test)[:,1])

svc=SVC(C=100.0)


# fit classifier to training set
svc.fit(os_data_X,os_data_y.values.ravel())


# make predictions on test set
y_pred=svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)

sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)


# compute and print accuracy score
print('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

svc=SVC(C=1000.0)


# fit classifier to training set
svc.fit(os_data_X,os_data_y.values.ravel())


# make predictions on test set
y_pred=svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)

# compute and print accuracy score
print('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

svc=SVC(C=5)


# fit classifier to training set
svc.fit(os_data_X,os_data_y.values.ravel())


# make predictions on test set
y_pred=svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)


# compute and print accuracy score
print('Model accuracy score with rbf kernel and C=5.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

"""### **Linear Kernel**"""

# instantiate classifier with linear kernel and C=1.0
linear_svc=SVC(kernel='linear', C=5.0)


# fit classifier to training set
linear_svc.fit(os_data_X,os_data_y.values.ravel())


# make predictions on test set
y_pred=linear_svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)


# compute and print accuracy score
print('Model accuracy score with linear kernel and C=5.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

"""### **Poly Kernel**"""

# instantiate classifier with polynomial kernel and C=1.0
poly_svc=SVC(kernel='poly', C=5.0)


# fit classifier to training set
poly_svc.fit(X_train,y_train)


# make predictions on test set
y_pred=poly_svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)


# compute and print accuracy score
print('Model accuracy score with polynomial kernel and C=5.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

"""### **Sigmoid Kernel**"""

# instantiate classifier with sigmoid kernel and C=1.0
sigmoid_svc=SVC(kernel='sigmoid', C=5.0)


# fit classifier to training set
sigmoid_svc.fit(os_data_X,os_data_y.values.ravel())


# make predictions on test set
y_pred=sigmoid_svc.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
sns.heatmap(conf_matrix, annot = True, fmt = 'g')
plt.title("Confussion Matrix", fontsize = 20)


# compute and print accuracy score
print('Model accuracy score with sigmoid kernel and C=5.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
print('Recall: %.3f' % recall_score(y_test, y_pred))
print('Precision: %.3f' % precision_score(y_test, y_pred))

"""### *one VS one*"""

# C values list
C = []
C.append(1)
for i in range(7):
    C.append(C[i]*2)
print(C)

from sklearn.multiclass import OneVsOneClassifier

c_recall = []
for c in C:
    ovo1 = OneVsOneClassifier((LinearSVC(random_state=0, C=c)).fit(X_train, y_train), n_jobs=6)
    modelovo1 = ovo1.fit(X_train, y_train)
    y_test_pred = modelovo1.predict(X_test)
    #accuracy = accuracy_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    c_recall.append(recall)



log_C_params = []
for i in C:
    c = math.log(i, 2)
    log_C_params.append(c)
plt.plot(log_C_params, c_recall, marker='o',linestyle=':', color='green')
plt.xlabel('LOG - C')
plt.ylabel('Recall')

SVM_OneOne=max(c_recall)
print(f"the maximum recall is {round(max(c_recall),4)}")

"""### *one VS rest*"""

c_recall_a = []
for c in C:
    ovo1 = OneVsRestClassifier((LinearSVC(random_state=0, C=c)).fit(X_train, y_train), n_jobs=4)
    modelovo1 = ovo1.fit(X_train, y_train)
    y_test_pred = modelovo1.predict(X_test)
    #accuracy = accuracy_score(y_test, y_test_pred)
    recall_a = recall_score(y_test, y_test_pred)
    c_recall_a.append(recall_a)

import math
log_C_params = []
for i in C:
    c = math.log(i, 2)
    log_C_params.append(c)
plt.plot(log_C_params, c_recall_a, marker='p',linestyle='--', color='green')
plt.xlabel('LOG - C')
plt.ylabel('Recall')

SVM_oneAll=max(c_recall_a)
print(f"the maximum recall is {round(max(c_recall_a),4)}")

"""###**Naive Bayes**

### *Gaussian*
"""

# Model specific Library
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
#X_train, X_test, y_train, y_test= train_test_split(os_data_X , os_data_y,test_size=0.2, random_state=42)

clf = GaussianNB()
clf.fit(os_data_X, os_data_y)
clf.score(X_test, y_test)

predicted = clf.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted)
NB=recall_score(y_test, predicted)
print('Accuracy: %.3f' % accuracy_score(y_test, predicted))
print('Recall: %.3f' % recall_score(y_test, predicted))
print('Precision: %.3f' % precision_score(y_test, predicted))
print(classification_report(y_test, predicted))
print(conf_matrix)

## multinominal
clf_mn_s = MultinomialNB()
clf_mn_s.fit(os_data_X, os_data_y)
clf_mn_s.score(X_test, y_test)

# Berouilli
clf_b = BernoulliNB()
clf_b.fit(os_data_X, os_data_y)
clf_b.score(X_test, y_test)

"""### **Additional experiment: Multiple Model**"""

from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC


X_train, X_test, y_train, y_test= train_test_split(os_data_X , os_data_y,test_size=0.2, random_state=42)
scoring = 'accuracy'
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# evaluate each model in turn
results = []
results_a = []
names_a = []
names = []
for name, model in models:
    kfold= model_selection.KFold(n_splits=10)
    cv_results= model_selection.cross_val_score(model, os_data_X, os_data_y, cv=kfold, scoring=scoring)
    cv_results_a= model_selection.cross_val_score(model, os_data_X, os_data_y, cv=kfold, scoring="recall")
    results.append(cv_results)
    results_a.append(cv_results_a)
    names_a.append(name)
    names.append(name)
    msg= "%s: %f (%f)" % (name+"-Accuracy", cv_results.mean(), cv_results.std())
    msg_a= "%s: %f (%f)" % (name+"-Recall",cv_results_a.mean(), cv_results_a.std())
    print(msg)
    print(msg_a)

"""### **KNN**"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(os_data_X, os_data_y)
predicted_knn = knn.predict(X_test)
conf_matrix = confusion_matrix(y_true=y_test, y_pred=predicted_knn)
KNN=recall_score(y_test, predicted_knn)
print(conf_matrix)
print('Accuracy: %.3f' % accuracy_score(y_test, predicted_knn))
print('Recall: %.3f' % recall_score(y_test, predicted_knn))
print('Precision: %.3f' % precision_score(y_test, predicted_knn))
print(classification_report(y_test, predicted_knn))

"""### **Comparing Model**"""

logistic=round(logistic*100,2)
SVM_RBF=round(SVM_RBF*100,2)
NB=round(NB*100,2)
KNN=round(KNN*100,2)
Base=round(Base*100,2)
SVM_oneAll=round(SVM_oneAll*100,2)
SVM_OneOne=round(SVM_OneOne*100,2)

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM_RBF', 'Naive Bayes','KNN','Baseline','SVM_OnevAll','SVM_OnevOne'],

    'Score': [logistic,SVM_RBF, NB,KNN,Base,SVM_oneAll,SVM_OneOne ]
    })

models.sort_values(by='Score', ascending=False)

"""### **RBF-SVM recall_score is high, So we select our main model as SVM(RBF Kernel)**

### *Feature Importance*
"""

# model = LogisticRegression()
# model.fit(X_train, y_train)
importances = pd.DataFrame(data={
    'Attribute': os_data_X.columns,
    'Importance': model.coef_[0]
})
importances = importances.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(35,4))
plt.bar(x=importances['Attribute'], height=importances['Importance'], color='#087E8B')
plt.title('Feature importances obtained from coefficients', size=20)
plt.xticks(rotation='vertical')
plt.show()

importances.set_index('Attribute',inplace=True)

pos_features = importances.loc[importances.Importance > 0]

num = np.min([50, len(pos_features)])
ylocs = np.arange(num)
# get the feature importance for top num and sort in reverse order
values_to_plot = pos_features.iloc[:num].values.ravel()[::-1]
feature_labels = list(pos_features.iloc[:num].index)[::-1]

plt.figure(num=None, figsize=(10, 5), dpi=80, facecolor='w', edgecolor='k');
plt.barh(ylocs, values_to_plot, align = 'center')
plt.ylabel('Features')
plt.xlabel('Importance Score')
plt.title('Positive Feature Importance Score - ExtraTrees Classifier')
plt.yticks(ylocs, feature_labels)
plt.show()

plt.bar(df_new.X160,df_new.y==1)
plt.show()

plt.bar(df_new.X26,df_new.y==1)
plt.show()

plt.bar(df_new.X171,df_new.y==1)
plt.show()

plt.bar(df_new.X161,df_new.y==1)
plt.show()

plt.bar(df_new.X133,df_new.y==1)
plt.show()



from sklearn.decomposition import PCA

pca = PCA().fit(X_train)

plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3, color='#087E8B')
plt.title('Cumulative explained variance by number of principal components', size=20)
plt.show()

print("It means we can explain 90-ish% of the variance in your source dataset with the first 35 principal components")

loadings = pd.DataFrame(
    data=pca.components_.T * np.sqrt(pca.explained_variance_),
    columns=[f'PC{i}' for i in range(1, len(X_train.columns) + 1)],
    index=X_train.columns
)
loadings.head()

# https://betterdatascience.com/feature-importance-python/

plt.figure(figsize=(35,4))
pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC1']]
pc1_loadings = pc1_loadings.reset_index()
pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']

plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')
plt.title('PCA loading scores (first principal component)', size=20)
plt.xticks(rotation='vertical')
plt.show()

plt.figure(figsize=(35,4))
pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC35']]
pc1_loadings = pc1_loadings.reset_index()
pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']

plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')
plt.title('PCA loading scores (first principal component)', size=20)
plt.xticks(rotation='vertical')
plt.show()

plt.figure(figsize=(35,4))
pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC50']]
pc1_loadings = pc1_loadings.reset_index()
pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']

plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')
plt.title('PCA loading scores (first principal component)', size=20)
plt.xticks(rotation='vertical')
plt.show()

plt.figure(figsize=(35,4))
pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC178']]
pc1_loadings = pc1_loadings.reset_index()
pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']

plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')
plt.title('PCA loading scores (first principal component)', size=20)
plt.xticks(rotation='vertical')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import numpy as np

x = np.linspace(0,1,100)
train, test, train_t, test_t = train_test_split(os_data_X, os_data_y, train_size=0.8)
model = LogisticRegression(n_jobs=-1)
model.fit(train, train_t)

plt.plot(*roc_curve(test_t, model.predict_proba(test)[:,1])[:2])
plt.plot(*roc_curve(test_t, model.predict(test))[:2])

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.plot(x, x, '-r')
plt.show()

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(test_t, model.predict_proba(test)[:,1])
print(auc)